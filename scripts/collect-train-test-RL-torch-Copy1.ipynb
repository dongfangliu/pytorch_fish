{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt      \n",
    "from mpl_toolkits import mplot3d\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_fish.envs.lib import pyflare as fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Experiment_name = 'servo frame skip 200 1.0 -45,-135'\n",
    "Experiment_note = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpuId = 1\n",
    "frame_skip = 200\n",
    "theta = np.array([-135,-45])\n",
    "vel_theta = np.array([-180,180])\n",
    "random_vel = np.array([0,0.3])\n",
    "radius=1.0\n",
    "max_time = 10\n",
    "action_max= 13\n",
    "done_dist = 0.15\n",
    "dist_distri_param =np.array([0,0.3])\n",
    "couple_mode =  fl.COUPLE_MODE.TWO_WAY\n",
    "ratio = 5/frame_skip*(max_time/10)\n",
    "use_com=False\n",
    "import torch\n",
    "torch.cuda.set_device(gpuId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr = 0.4*np.array([1.0,0.5])\n",
    "wp = 1.0*np.array([0.0,1.0])\n",
    "wa = 0\n",
    "live_penality =-0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_train_steps= int(2e6*ratio)\n",
    "replay_buffer_capacity=int(2e6*ratio)\n",
    "num_seed_steps=int(2048*ratio)\n",
    "eval_frequency=int(20000*ratio)\n",
    "num_eval_episodes=1\n",
    "device=\"cuda\"\n",
    "# logger\n",
    "log_frequency=1\n",
    "log_save_tb=True\n",
    "# video recorder\n",
    "save_video=False\n",
    "seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "discount=0.99\n",
    "init_temperature=0.1\n",
    "alpha_lr=3e-4\n",
    "alpha_betas=[0.9, 0.999]\n",
    "actor_lr=3e-4\n",
    "actor_betas=[0.9, 0.999]\n",
    "actor_update_frequency=1\n",
    "critic_lr=3e-4\n",
    "critic_betas=[0.9, 0.999]\n",
    "critic_tau=0.005\n",
    "critic_target_update_frequency=2\n",
    "batch_size=int(2048*ratio)\n",
    "learnable_temperature=True\n",
    "\n",
    "# critic settings\n",
    "critic_hidden_dim=128\n",
    "critic_hidden_depth=2\n",
    "# actor settings\n",
    "actor_hidden_depth=2\n",
    "actor_hidden_dim=128\n",
    "actor_log_std_bounds=[-5, 2]\n",
    "\n",
    "evaluate_save_data = False\n",
    "evaluate_save_fluid = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/pytorch_fish\n"
     ]
    }
   ],
   "source": [
    "os.chdir(Path(os.getcwd()+\"/../\").resolve())\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()+\"/\"\n",
    "data_folder =cwd+ 'py_data/'\n",
    "json_folder =data_folder+'jsons/'\n",
    "path_folder =json_folder+'paths/'\n",
    "scripts_folder = cwd+'scripts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/pytorch_fish/py_data/jsons/fluid_param_1.0.json\n",
      "/root/pytorch_fish/py_data/jsons/rigids_4_30.json\n",
      "/root/pytorch_fish/py_data/jsons/paths/line.json\n"
     ]
    }
   ],
   "source": [
    "fluid_json = json_folder+'fluid_param_1.0.json'\n",
    "rigid_json = json_folder+'rigids_4_30.json'\n",
    "path_json = str(Path(\"./py_data/jsons/paths/line.json\").resolve())\n",
    "print(fluid_json)\n",
    "print(rigid_json)\n",
    "print(path_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_infos(infos,title,reward_fig_name):\n",
    "    dd = {k:[] for k in infos[0].keys()}\n",
    "    for info in infos:\n",
    "        for k in info.keys():\n",
    "            dd[k].append(info[k])\n",
    "    plt.figure()\n",
    "    for k in dd.keys():\n",
    "        if k=='action_penality':\n",
    "            plt.plot(np.arange(0,len(dd[k])),[kkk  for kkk in dd[k]],label=k)\n",
    "        else:\n",
    "            plt.plot(np.arange(0,len(dd[k])),dd[k],label=k)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.savefig(reward_fig_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store_folder_name = time.strftime('%Y-%m-%d/',time.localtime(time.time()))+Experiment_name+'_'+time.strftime('%Y-%m-%d %H:%M/',time.localtime(time.time()))\n",
    "imgs_folder_name = 'imgs/'\n",
    "rl_data_folder_name = 'rl_data/'\n",
    "rl_data_all_name='collected_o_a.npz'\n",
    "network_folder_name = 'networks/'\n",
    "store_folder = cwd+'output_data/'+store_folder_name\n",
    "imgs_folder = store_folder+imgs_folder_name\n",
    "rl_data_folder= store_folder+rl_data_folder_name\n",
    "network_folder = store_folder+network_folder_name\n",
    "\n",
    "if not os.path.exists(store_folder):\n",
    "    os.makedirs(store_folder)\n",
    "if not os.path.exists(imgs_folder):\n",
    "    os.makedirs(imgs_folder)\n",
    "if not os.path.exists(rl_data_folder):\n",
    "    os.makedirs(rl_data_folder)\n",
    "if not os.path.exists(network_folder):\n",
    "    os.makedirs(network_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_sources = True\n",
    "sources_folder_name = ['py_data','scripts']\n",
    "if save_sources==True:\n",
    "    for fn in sources_folder_name:\n",
    "        shutil.copytree(cwd+fn,store_folder+'sources/'+fn)\n",
    "with open(store_folder+'note.txt','w+') as f:\n",
    "    f.write(time.strftime('%Y-%m-%d %H:%M\\n',time.localtime(time.time())))\n",
    "    f.write(Experiment_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL_TRAINING Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "from sac.logger import Logger\n",
    "from sac.agent.replay_buffer import ReplayBuffer\n",
    "from sac.agent import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sac.agent.sac import SACAgent\n",
    "from sac.agent.critic import DoubleQCritic\n",
    "from sac.agent.actor import DiagGaussianActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.98635312e-01, -6.46525442e-04, -5.22215937e-02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym_fish.envs import FishEnv\n",
    "env=gym.make('fish-v0', \n",
    "                  gpuId=gpuId,\n",
    "                        couple_mode=couple_mode,frame_skip=frame_skip,radius=radius,\n",
    "                       theta=theta,action_max=action_max,random_vel=random_vel,max_time=max_time,\n",
    "                       fluid_json=fluid_json,wp=wp,wr=wr,wa=wa,live_penality =live_penality,\n",
    "                       rigid_json=rigid_json,vel_theta = vel_theta,\n",
    "                       done_dist=done_dist,dist_distri_param=dist_distri_param,use_com=use_com\n",
    "               \n",
    "                   )\n",
    "# agent settings\n",
    "obs_dim=env.observation_space.shape[0] # to be specified later\n",
    "action_dim=env.action_space.shape[0] # to be specified later\n",
    "action_range=[\n",
    "        float(env.action_space.low.min()),\n",
    "        float(env.action_space.high.max())\n",
    "    ] # to be specified later\n",
    "env.rigid_data.skeletons[0].dynamics.getBaseLinkFwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = 'SAC'\n",
    "algofoler=algo+'/'\n",
    "if not os.path.exists(network_folder+algofoler):\n",
    "    os.makedirs(network_folder+algofoler)\n",
    "if not os.path.exists(network_folder+algofoler+'imgs/'):\n",
    "    os.makedirs(network_folder+algofoler+'imgs/')\n",
    "if not os.path.exists(network_folder+algofoler+'imgs/trajs/'):\n",
    "    os.makedirs(network_folder+algofoler+'imgs/trajs/')\n",
    "if not os.path.exists(network_folder+algofoler+'imgs/rewards/'):\n",
    "    os.makedirs(network_folder+algofoler+'imgs/rewards/')\n",
    "if not os.path.exists(network_folder+algofoler+'models/'):\n",
    "    os.makedirs(network_folder+algofoler+'models/')\n",
    "\n",
    "tb_folder =  network_folder+algofoler\n",
    "model_folder = network_folder+algofoler+'models/'\n",
    "result_img_folder = network_folder+algofoler+'imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger = Logger(tb_folder,\n",
    "                             save_tb=log_save_tb,\n",
    "                             log_frequency=log_frequency,\n",
    "                             agent=\"sac\")\n",
    "critic_network = DoubleQCritic(obs_dim,action_dim,critic_hidden_dim,critic_hidden_depth)\n",
    "\n",
    "actor_network = DiagGaussianActor(obs_dim,action_dim,actor_hidden_dim,actor_hidden_depth,actor_log_std_bounds)\n",
    "\n",
    "replay_buffer = ReplayBuffer(env.observation_space.shape,env.action_space.shape,int(replay_buffer_capacity),device)\n",
    "\n",
    "agent = SACAgent(obs_dim,action_dim,action_range,device,critic_network,actor_network,replay_buffer,discount,init_temperature,alpha_lr,alpha_betas,actor_lr,actor_betas,actor_update_frequency,critic_lr,critic_betas,critic_tau,critic_target_update_frequency,batch_size,learnable_temperature)\n",
    "\n",
    "utils.set_seed_everywhere(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "next_eval_steps= eval_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " def collect_traj(seed_steps=False):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        reward = 0\n",
    "        samples = []\n",
    "        while not done:\n",
    "            if seed_steps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with utils.eval_mode(agent):\n",
    "                        action = agent.act(obs, sample=True)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            samples.append((obs,action,next_obs,reward,done))\n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                if (not np.isfinite(obs).all()):\n",
    "                    print('bad observation, try new traj')\n",
    "                    samples.clear()\n",
    "                    obs = env.reset()\n",
    "                    done = False\n",
    "                    reward = 0 \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_seed_samples():\n",
    "        step = 0 \n",
    "        while step<num_seed_steps:\n",
    "            samples = collect_traj(seed_steps=True)\n",
    "            for sample in samples:\n",
    "                obs,action,next_obs,reward,done = sample\n",
    "                agent.replay_buffer.add(obs, action, reward, next_obs,done,done)\n",
    "                step+=1\n",
    "                if step>=num_seed_steps:\n",
    "                    break\n",
    "        print(\"Collect seed samples complete. Step: \",step,\" Seed steps:\",num_seed_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "        average_episode_reward = 0\n",
    "        reward_fig_name =result_img_folder+'rewards/'+\"steps_{0}.png\".format(step)\n",
    "        traj_fig_name =result_img_folder+'trajs/'+\"steps_{0}.png\".format(step)\n",
    "        best_rewards = -9999\n",
    "        for episode in range(num_eval_episodes):\n",
    "            print(\"Evaluating episode \" + str(episode) +\" ....\")\n",
    "            obs = env.reset()\n",
    "            agent.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            infos = []\n",
    "            while not done:\n",
    "                with utils.eval_mode(agent):\n",
    "                    action = agent.act(obs, sample=False)\n",
    "                if evaluate_save_data:\n",
    "                    obs, reward, done, info = env.stepSave(action,save_fluid=evaluate_save_fluid)\n",
    "                else:\n",
    "                    obs, reward, done, info = env.step(action)\n",
    "                infos.append(info)\n",
    "                # video_recorder.record(env)\n",
    "                episode_reward += reward\n",
    "            if episode_reward>best_rewards:\n",
    "                best_rewards = episode_reward\n",
    "                plot_infos(infos,title=str(step),reward_fig_name=reward_fig_name)\n",
    "                env.plot3d(title=str(step),fig_name = traj_fig_name)\n",
    "            average_episode_reward += episode_reward\n",
    "            # video_recorder.save(f'{step}.mp4')\n",
    "        average_episode_reward /= num_eval_episodes\n",
    "        logger.log('eval/episode_reward', average_episode_reward,step)\n",
    "        logger.dump(step,ty='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove './data/vis_data/Objects/*': No such file or directory\r\n",
      "rm: cannot remove './data/vis_data/Fluid/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!./clean_visdata.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_seed_samples()\n",
    "episode, episode_reward= 0, 0\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while step < num_train_steps:\n",
    "    samples = collect_traj()\n",
    "    for sample in samples:\n",
    "        obs,action,next_obs,reward,done = sample\n",
    "        if done:\n",
    "            logger.log('train/duration',time.time() - start_time, step)\n",
    "            start_time = time.time()\n",
    "            # evaluate agent periodically\n",
    "            if step >=next_eval_steps:\n",
    "                # save model\n",
    "                agent.save(model_folder+\"steps_{0}\".format(step))\n",
    "                logger.log('eval/episode', episode, step)\n",
    "                next_eval_steps = step+eval_frequency\n",
    "                evaluate()\n",
    "            logger.log('train/episode_reward', episode_reward,step)\n",
    "            episode_reward = 0\n",
    "            episode += 1\n",
    "            logger.log('train/episode', episode, step)\n",
    "            logger.dump(step, save=True,ty='train')\n",
    "        # run training update\n",
    "        agent.update(logger, step)  \n",
    "        done = float(done)\n",
    "        episode_reward += reward\n",
    "        step += 1\n",
    "        agent.replay_buffer.add(obs, action, reward, next_obs, done,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'buffer':agent.replay_buffer},model_folder+'buffer')\n",
    "torch.save({'actor':agent.actor.state_dict()},model_folder+'actor')\n",
    "agent.save(model_folder+'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_save_fluid=True\n",
    "evaluate_save_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_traj(theta,dist):\n",
    "        print(\"Evaluating episode  ....\"+str(theta))\n",
    "        obs = env.reset()\n",
    "        env.set_task(theta,dist)\n",
    "        ref_line = fl.debugLine()\n",
    "        ref_line.vertices = [\n",
    "                    env.path_start*(1.0-t)+env.goal_pos*t for t in np.arange(0.0,1.0,1.0/100)\n",
    "                ]\n",
    "        fl.VTKWriter.writeLines([ref_line], env.dataPath[\"trajectory\"]+\"/trajectory_ideal.vtk\")\n",
    "        agent.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        infos = []\n",
    "        while not done:\n",
    "            with utils.eval_mode(agent):\n",
    "                action = agent.act(obs, sample=False)\n",
    "            if evaluate_save_data:\n",
    "                obs, reward, done, info = env.stepSave(action,save_fluid=evaluate_save_fluid)\n",
    "            else:\n",
    "                obs, reward, done, info = env.step(action)\n",
    "            infos.append(info)\n",
    "            # video_recorder.record(env)\n",
    "            episode_reward += reward\n",
    "        print(episode_reward)\n",
    "#         plot_infos(infos,title=str(theta))\n",
    "        env.plot3d(title=str(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./clean_visdata.sh\n",
    "evaluate_traj(0,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ref_line = fl.debugLine()\n",
    "        ref_line.vertices = [\n",
    "                    env.path_start*(1.0-t)+env.goal_pos*t for t in np.arange(0.0,1.0,1.0/100)\n",
    "                ]\n",
    "        fl.VTKWriter.writeLines([ref_line], env.dataPath[\"trajectory\"]+\"/trajectory_ideal.vtk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import FancyArrowPatch\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.proj3d import proj_transform\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, x, y, z, dx, dy, dz, *args, **kwargs):\n",
    "        super().__init__((0,0), (0,0), *args, **kwargs)\n",
    "        self._xyz = (x,y,z)\n",
    "        self._dxdydz = (dx,dy,dz)\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        x1,y1,z1 = self._xyz\n",
    "        dx,dy,dz = self._dxdydz\n",
    "        x2,y2,z2 = (x1+dx,y1+dy,z1+dz)\n",
    "\n",
    "        xs, ys, zs = proj_transform((x1,x2),(y1,y2),(z1,z2), renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        super().draw(renderer)\n",
    "def _arrow3D(ax, x, y, z, dx, dy, dz, *args, **kwargs):\n",
    "    '''Add an 3d arrow to an `Axes3D` instance.'''\n",
    "    arrow = Arrow3D(x, y, z, dx, dy, dz, *args, **kwargs)\n",
    "    ax.add_artist(arrow)\n",
    "setattr(Axes3D,'arrow3D',_arrow3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_fish.envs.py_util import flare_util\n",
    "from gym_fish.envs.lib import pyflare as fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fluid_json = json_folder+'fluid_param_0.5.json'\n",
    "# rigid_json = json_folder+'rigids_4_30_test.json'\n",
    "# frame_skip = 100\n",
    "from gym_fish.envs import FishEnv\n",
    "env=gym.make('fish-v0', \n",
    "                  gpuId=gpuId,\n",
    "                        couple_mode=couple_mode,frame_skip=frame_skip,radius=radius,\n",
    "                       theta=theta,action_max=action_max,random_vel=random_vel,max_time=max_time,\n",
    "                       fluid_json=fluid_json,wp=wp,wr=wr,wa=wa,live_penality =live_penality,\n",
    "                       rigid_json=rigid_json,vel_theta = vel_theta,\n",
    "                       done_dist=done_dist,dist_distri_param=dist_distri_param,use_com=use_com\n",
    "               \n",
    "                   )\n",
    "env.plot3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test generalization to sequence goals\n",
    "path = flare_util.path_data()\n",
    "path.from_json(str(Path(\"./py_data/jsons/paths/path_ss.json\").resolve()))\n",
    "traj  =  path.trajectory\n",
    "paths_by_t= np.array([ traj.getPose(t).getPosition()  for t in np.arange(0.0,1.0,1.0/100) ])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "X = paths_by_t[:,0]\n",
    "Y = paths_by_t[:,1]\n",
    "Z = paths_by_t[:,2]\n",
    "ax.scatter3D(xs=X, zs=Y, ys=Z,c=[[0,i/len(paths_by_t),0] for i in range(paths_by_t.shape[0])])\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('z')\n",
    "ax.set_zlabel('y')\n",
    "max_range = np.array([X.max()-X.min(), Y.max()-Y.min(), Z.max()-Z.min()]).max() / 2.0\n",
    "mid_x = (X.max()+X.min()) * 0.5\n",
    "mid_y = (Y.max()+Y.min()) * 0.5\n",
    "mid_z = (Z.max()+Z.min()) * 0.5\n",
    "ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "ax.set_ylim(mid_z - max_range, mid_z + max_range)\n",
    "ax.set_zlim(mid_y - max_range, mid_y + max_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_env(env,ax):\n",
    "    ax.clear()\n",
    "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
    "    ax.set_ylim(mid_z - max_range, mid_z + max_range)\n",
    "    ax.set_zlim(mid_y - max_range, mid_y + max_range)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('z')\n",
    "    ax.set_zlabel('y')\n",
    "    ax.view_init(elev=45,azim=45)#改变绘制图像的视角,即相机的位置,azim沿着z轴旋转，elev沿着y轴\n",
    "    dxyz = env.goal_pos-env.body_xyz\n",
    "    fwd = env.rigid_data.skeletons[0].dynamics.getBaseLinkFwd()\n",
    "    right = env.rigid_data.skeletons[0].dynamics.getBaseLinkRight()\n",
    "    up = env.rigid_data.skeletons[0].dynamics.getBaseLinkUp()\n",
    "    ax.arrow3D(env.body_xyz[0],env.body_xyz[2],env.body_xyz[1],fwd[0],fwd[2],fwd[1],arrowstyle=\"-|>\",ec ='green',mutation_scale=20,\n",
    "           fc='red')\n",
    "    ax.arrow3D(env.body_xyz[0],env.body_xyz[2],env.body_xyz[1],right[0],right[2],right[1],arrowstyle=\"-|>\",ec ='green',mutation_scale=20,\n",
    "           fc='green')\n",
    "    ax.arrow3D(env.body_xyz[0],env.body_xyz[2],env.body_xyz[1],up[0],up[2],up[1],arrowstyle=\"-|>\",ec ='green',mutation_scale=20,\n",
    "           fc='blue')\n",
    "    ax.arrow3D(env.body_xyz[0],env.body_xyz[2],env.body_xyz[1],dxyz[0],dxyz[2],dxyz[1],arrowstyle=\"-|>\",ec ='green',mutation_scale=20,\n",
    "           fc='yellow',)\n",
    "    ax.scatter3D(xs=X, zs=Y, ys=Z,c=[[0,i/len(paths_by_t),0] for i in range(paths_by_t.shape[0])])\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./clean_visdata.sh\n",
    "fl.VTKWriter.writeTrajectory(traj, env.dataPath['trajectory'] + '/trajectory.vtk')\n",
    "env.reset()\n",
    "env.training = False\n",
    "skeleton_dynamics = env.rigid_data.skeletons[0].dynamics\n",
    "skeleton_dynamics.getJoint(\"head\").setVelocity(0,0)\n",
    "skeleton_dynamics.getJoint(\"head\").setVelocity(1,0)\n",
    "joint_list =['spine','spine01','spine02','spine03']\n",
    "for jnt_name in joint_list:\n",
    "    skeleton_dynamics.getJoint(jnt_name).setPosition(0,0)\n",
    "    skeleton_dynamics.getJoint(jnt_name).setVelocity(0,0)\n",
    "\n",
    "t = 0.001\n",
    "dt = 0.03\n",
    "\n",
    "startPose = traj.getPose(t)\n",
    "path_position_begin = startPose.getPosition()\n",
    "path_orientation_begin = startPose.getOrientation()\n",
    "\n",
    "skeleton_dynamics.setHead(path_position_begin,path_orientation_begin)\n",
    "skeleton_dynamics.update()\n",
    "\n",
    "env.goal_pos = traj.getPose(traj.getReferencePose(env.body_xyz)+dt).getPosition() \n",
    "env.path_start  = env.body_xyz \n",
    "env.path_dir = env.goal_pos-env.path_start\n",
    "env.path_dir = env.path_dir/np.linalg.norm(env.path_dir)\n",
    "        \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.ion()\n",
    "actions= []\n",
    "observations = []\n",
    "vels = []\n",
    "while True:\n",
    "    if env.rigid_data.rigidWorld.time>100 or traj.getReferencePose(env.body_xyz) >0.98:\n",
    "        print(\"Time exceed! \",t,traj.getReferencePose(env.body_xyz),env.rigid_data.rigidWorld.time)\n",
    "        break \n",
    "    obs = env._get_obs()\n",
    "    observations.append(obs)\n",
    "    with utils.eval_mode(agent):\n",
    "        action = agent.act(obs, sample=False)\n",
    "    actions.append(action)\n",
    "    vels.append(env.rigid_data.skeletons[0].dynamics.getVelocities(includeBase=False))\n",
    "    env.stepSave(action,save_objects=False,save_fluid=False,test_mode=True)\n",
    "    \n",
    "    rela_vec_to_goal = env.goal_pos-env.body_xyz\n",
    "    dist_to_path = np.linalg.norm(rela_vec_to_goal-env.path_dir*np.dot(rela_vec_to_goal,env.path_dir))\n",
    "    env.proj_pt_world = env.goal_pos-env.path_dir*np.dot(rela_vec_to_goal,env.path_dir)\n",
    "    if np.linalg.norm(env.body_xyz-env.goal_pos)<0.2:\n",
    "        t = t+dt\n",
    "        env.goal_pos = traj.getPose(t).getPosition() \n",
    "        env.path_start  = env.body_xyz \n",
    "        env.path_dir = env.goal_pos-env.path_start\n",
    "        env.path_dir = env.path_dir/np.linalg.norm(env.path_dir)\n",
    "    plot_env(env,ax)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0,len(vels),1),vels)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,len(actions),1),actions)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,len(vels),1),np.array(actions)/np.array(vels)[:,:4])\n",
    "plt.show()\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(0,len(observations),1),np.array(observations))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./clean_visdata.sh\n",
    "fl.VTKWriter.writeTrajectory(env.path_data.trajectory, env.dataPath['trajectory'] + '/trajectory.vtk')\n",
    "env.reset()\n",
    "dones = False\n",
    "infos = []\n",
    "reward = 0\n",
    "env.rigid_data.skeletons[0].dynamics.getJoint(\"head\").setVelocity(0,0)\n",
    "env.rigid_data.skeletons[0].dynamics.getJoint(\"head\").setVelocity(1,0)\n",
    "\n",
    "t = 0.001\n",
    "dt = 0.08\n",
    "startPose = traj.getPose(t)\n",
    "path_position_begin = startPose.getPosition()\n",
    "path_orientation_begin = startPose.getOrientation()\n",
    "\n",
    "env.init_pos = path_position_begin\n",
    "env.set_theta(0)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.ion()\n",
    "    \n",
    "\n",
    "actions= []\n",
    "observations = []\n",
    "vels = []\n",
    "while not dones:\n",
    "    obs = env._get_obs()\n",
    "    observations.append(obs)\n",
    "    if env.rigid_data.rigidWorld.time>20:\n",
    "#     if env.rigid_data.rigidWorld.time>100 or traj.getReferencePose(env.body_xyz) >0.98:\n",
    "        print(\"Time exceed! \",t,traj.getReferencePose(env.body_xyz),env.rigid_data.rigidWorld.time)\n",
    "        break\n",
    "    with utils.eval_mode(ws.agent):\n",
    "        action = ws.agent.act(obs, sample=False)\n",
    "    actions.append(action)\n",
    "    vels.append(env.rigid_data.skeletons[0].dynamics.getVelocities(includeBase=False))\n",
    "    _,_,_,_=env.stepSave(action,save_objects=False,save_fluid=False)\n",
    "    env.goal_pos = env.body_xyz+env.goal_dir*2.0\n",
    "    plot_env(env,ax)\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0,len(vels),1),np.array(actions)/np.array(vels)[:,:4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
